---
title: "Draft CCVA: Bryce Canyon National Park"
author: "Caitlin Mothes and Katie Willi"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: paper
---


Regarding the water balance report, I think they have a nice approach using lag correlations but would suggest some modifications:

-Don't use moving averages of P, as this doesn't make sense physically. You could try correlations of the groundwater level or spring discharge (where relevant) with P at different time intervals and time lags: monthly, seasonal, annual time steps; lags 0-11 months

-This same correlation analysis can be done with P-PET or with a water balance model variable. That variable could be soil moisture if the soil moisture storage is always < full.

Regarding the model code, so exciting that this is already developed. For ET use Penman Monteith always, unless there's some reason not to.

For the water balance model, they are using the Jennings et al. paper for rain/snow temperature threshold - this is ok but is something that can be modified for areas that are snowy. I'm not sure if they used daily data in the Jennings paper; think it was sub-daily (KW: THAT IS CORRECT, THEY USED 3- and 6- HOUR DATA. The temperature thresholds vary depending on the time step; rain-snow separation is also sensitive to humidity.

Another part of the snow model that can be modified if needed is the hock melt factors - those can be calibrated. Most likely you won't have snow data to calibrate to, but can at least compare snow on/off timing to MODIS snow cover.

function get_soil has an equation for the rate of soil moisture depletion - this is also something that could be modified depending on site characteristics. I would be interested to know their source for the equation

function get_runoff would allow modifying direct runoff. this function could also be modified to generate runoff more gradually. looks like now it is assigning all water to runoff after accounting for soil moisture and AET. This quantity could instead contribute to an 'available for runoff' storage quantity. That quantity could be gradually changing, meaning not all of it is delivered to runoff each time step. Instead, some fraction of it can be delivered to runoff, recharge, or divided between the two. 



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      error = FALSE,
                      message = FALSE)
```

This workflow explores correlations between NPS water balance model (NPS WBM) variables, stream flow, and groundwater levels in and around Bryce Canyon National Park.

## Dowloading NPS WBM data

Here, we are delineating the watersheds that flow through BRCA and additional locations that have on-the ground data to compare the NPS WBM outputs to. With these areas of interest, we can pull the associated gridded NPS WBM data (from [Which Water Balance Model Do You Need?](https://screenedcleanedsummaries.s3.us-west-2.amazonaws.com/which_water_balance.html) - Historical THREDDS).



```{r}
# Set-up code: delineating all potential areas of interest (AOIs) associated with the park.
# Currently includes: park boundary; watersheds that cross park boundary; watersheds of nearby (100 km) ref gages; watersheds of manually-selected areas such as water supply locations

source("setup.R")

unit <- "BRCA"
##### Watersheds of the park

# get park boundary
park_boundary <- getParkBoundary(park = unit)

# pull in the park watersheds as a starting AOI for pulling in WBM data:
park_watershed <- getWatersheds(aoi = park_boundary, save = FALSE)

##### Watersheds of park water supplies

#NPS water supply locations may not necessarily be within the park, and therefore their watersheds may not be represented in the park boundary's watersheds delineated above. So, here I am pulling in the water supply locations of interest, and delineating their own watersheds to ensure we capture them in the WBM pull. Water supply locations are found using the [Utah Division of Water Rights database](https://maps.waterrights.utah.gov/asp/wrplatGE.asp).

# get Utah points of diversion near BRCA
POD_Utah <- getPODUtah(aoi = park_watershed, dist = 0.1)

# filter to water supplies of interest
POD_park <- POD_Utah %>%
  dplyr::filter(WRNUM %in% c("61-893", "2061001M00", # current supply
                             "61-1143")) %>% # potential future supply
  distinct(LOCATION, .keep_all = TRUE) 

watersupply_watershed <- vector("list", nrow(POD_park))

# get the watersheds of different points:
for(i in 1:nrow(POD_park)){
  
  watersupply_watershed[[i]] <- POD_park[i,] %>% 
    getXYWatersheds(sf = ., coordinates = NULL)
  
}

watersupply_watershed <- watersupply_watershed %>%
  bind_rows() %>% 
  distinct(featureid, .keep_all = TRUE) %>%
  #... hard to tell from topo what the future supply's real watershed looks like... 
  # But! This future supply is a deep (100 ft) well, so it may not be as important.
  # I'll need to look into this further at some point.
  filter(!featureid %in% c("10807991", "10807997"))

##### Watersheds for nearby USGS stream gages

#For many parks, the nearest USGS stream gage may still be far away. Therefore, we are pulling all NWIS gages within 100 km of the park boundary. Of those, we only select stream gages that are considered "reference" gages in the [GAGES-II database (Falcone, 2011)](https://pubs.usgs.gov/publication/70046617) and have data in our time period of interest (1980-2022). Then, we delineate each of those gages' watersheds using the `get_nldi_basin()` function from the {nhdplusTools} package:

# area around the park ~ 100 km away:
aoi <- sf::st_buffer(park_boundary, dist = 0.3)
# locate NWIS daily flow locations in that radius:
nwis <- listNWIS(aoi = aoi, dist = 0) %>%
  # daily values...
  filter(data_type_cd == "dv",
         # ... of flow
         code == "00060") %>%
  # for comparison purposes, need data from 1980 onwards
  filter(year(end_date) >= 1980)

# find the NWIS gages that are (crudely, probably) representative of natural conditions:
ref_gages <- get_gagesII(id = nwis$site_no) %>%
  filter(class == "Ref")

nwis <- nwis %>%
  filter(site_no %in% ref_gages$staid) %>%
  left_join(st_drop_geometry(ref_gages), by = c("site_no" ="staid"))

# for(i in 1:nrow(nwis)){
#   nwis$comid[i] <- discover_nhdplus_id(nwis[i,])
# }

# pull those sites' flow data
getNWIS(inventory = nwis, park = "misc", path = "data/")

# where are they?
mapview(nwis) + mapview(park_boundary)

nldi_finder <- function(site_no){
  # Now, get those gages' watersheds using get_nldi_basin in the {nhdplusTools}.
  # Input for NLDI requires "USGS-" before the gage number
  nldi_nwis <- list(featureSource = "nwissite",
                    featureID = paste0("USGS-", site_no))
  
  gage_basin <- nhdplusTools::get_nldi_basin(nldi_feature = nldi_nwis) %>%
    st_transform(., 4269) %>%
    mutate(site_no = site_no)
  
  return(gage_basin)
  
}

nldi_meta <- function(site_no){
  # Now, get those gages' watersheds using get_nldi_basin in the {nhdplusTools}.
  # Input for NLDI requires "USGS-" before the gage number
  nldi_nwis <- list(featureSource = "nwissite",
                    featureID = paste0("USGS-", site_no))
  
  gage_basin <- nhdplusTools::get_nldi_characteristics(nldi_feature = nldi_nwis, type = "total")[[1]] %>%
    filter(characteristic_id %in% c("TOT_ELEV_MEAN", "TOT_ELEV_MAX", "TOT_ELEV_MIN")) %>%
    pivot_wider(-percent_nodata, values_from = characteristic_value, names_from = characteristic_id)
  
  return(gage_basin)
  
}

nldi_watershed <- nwis$site_no %>%
  map_dfr(~nldi_finder(site_no = .)) %>%
  mutate(data = map(site_no, ~nldi_meta(site_no = .))) %>%
  unnest() %>%
  left_join(st_drop_geometry(nwis), by = "site_no")

mapview(nldi_watershed) + mapview(watersupply_watershed) + mapview(park_watershed) + mapview(park_boundary)

# Lastly, we dissolve all of these watersheds into a single shapefile that we can use to download the gridded monthly water balance predictions (from 1980-2022). The function below downloads and crops the gridded datasets to this area of interest, then converts the information into a table. The coordinates of the grid centroids are preserved. For all variables across all years, this takes a little under an hour.

# our AOI blob!
final_aoi <- park_watershed %>%
  bind_rows(watersupply_watershed) %>%
  bind_rows(nldi_watershed) %>%
  bind_rows(park_boundary) %>%
  summarize() %>%
  mutate(park = unit)
# rm(list=setdiff(ls(), "final_aoi"))
# options(timeout = 900)
# getHistoricWBM(park = "misc", aoi = final_aoi, path = "data/", wb_vars = c("soil_water", "runoff", "rain", "accumswe", "PET", "Deficit", "AET"))
```

### Well data

In ~1998 (?), a pump test was conducted on Bryce Canyon's current water supply system that resulted in the following approximations for aquifer characteristics:

-	Hydraulic conductivity ratio 10/1 (?)
-	Transmissivity (T) = 8900 ft^2/day
-	Specific Yield (Sy) = 0.37
-	Kr = 150 feet/day
-	Aquifer thickness = 60 feet
-	Hydraulic gradient between well 1 and 2 = 0.0057 ft/ft
-	Hydraulic gradient upstream of well field = 0.0042 ft/ft
-	Average daily use = 112,000 gallons/day (or, 15,000 cubic feet/day) … summer pumpage… but greatly overestimates winter use.

At around that same time, they began measuring static water levels in their water supply wells at roughly a weekly frequency. 

```{r}
well_data_raw <- read_csv('data/park/BRCA/manual/BRCA_Well_Data.csv') 
names(well_data_raw) <- make.names(names(well_data_raw))

well_monthly <- well_data_raw %>%
  dplyr::mutate(static_in = as.numeric(str_replace(Static..in., "-", "")),
                date = mdy(Date),
                ym = ym(substr(date, 1, 7))) %>%
  dplyr::select(date,
                ym,
                well = Well,
                static_in) %>%
  dplyr::group_by(well, ym) %>%
  dplyr::summarize(monthly_static = mean(static_in, na.rm = TRUE))

well_daily <- well_data_raw %>%
  dplyr::mutate(static_in = as.numeric(str_replace(Static..in., "-", "")),
                date = mdy(Date),
                ym = ym(substr(date, 1, 7))) %>%
  dplyr::select(date,
                well = Well,
                static_in)

# Find how much data is available during the time period of each site:
well_daily %>%
  group_by(well) %>%
  filter(!is.na(static_in)) %>%
  summarize(start = min(date), #start date
            end = max(date), #end date
            # how many days exist between the start and end date
            possible_n = (as.numeric(ymd(max(date)) - ymd(min(date))) + 1),
            # how many days we have a record for:
            actual_n = n()) %>%
  mutate(percent_daily = 100 * (actual_n/possible_n)) %>%
  select(well, start, end, percent_daily) %>%
  cbind(., well_monthly %>%
          mutate(count = ifelse(is.na(monthly_static), 0, 1)) %>%
          group_by(well) %>%
          summarize(percent_monthly = 100 * (sum(count)/n())) %>%
          select(percent_monthly)
  ) %>%
  DT::datatable(filter = "none",
                caption = 'Fraction of available daily data at BRCA park wells.',
                rownames = FALSE,
                fillContainer = T,
                escape = FALSE,
                options = list(dom = 't',
                               pageLength = nrow(.),
                               scrollY = '300px')) 
```

```{r}
ggplotly(ggplot(data = well_daily) +
           geom_line(aes(x=date, y=static_in)) +
           facet_wrap(~well, nrow = 2) +
           theme_bw() +
           xlab("Date") +
           ylab("Static Level (in)"))
```

Based on information gleaned from visualizing the static water level data, it appears that water levels fluctuate every five years 


Each month, the park must report on water use to the state of Utah. When we compare this information to static level.

```{r}
water_supply <- getWaterSuppliersUtah(aoi = park_boundary) %>%
  filter(grepl("National Park", WRNAME, ignore.case=TRUE)) %>%
  .$WRID

water_use_1 <- getWaterUseUtah(WRID = water_supply)[[1]] %>%
  slice(1:39) %>%
  pivot_longer(-c("Year", "Method of Measurement"), names_to = "month", values_to = "use_acre_feet") %>%
  mutate(ym = ym(paste0(Year, "-", month))) %>%
  filter(month != "Annual inAcre Feet") %>%
  select(ym, use_acre_feet) %>%
  mutate(well = "Well 1")

water_use_2 <- getWaterUseUtah(WRID = water_supply)[[1]] %>%
  slice(51:nrow(.)) %>%
  dplyr::filter(!is.na(as.numeric(Year))) %>%
  pivot_longer(-c("Year", "Method of Measurement"), names_to = "month", values_to = "use_acre_feet") %>%
  mutate(ym = ym(paste0(Year, "-", month))) %>%
  filter(month != "Annual inAcre Feet") %>%
  select(ym, use_acre_feet) %>%
  mutate(well = "Well 2")

both_wells <- water_use_1 %>%
  bind_rows(water_use_2)

joined_well_data <- well_monthly %>%
  left_join(both_wells, by = c("ym", "well")) %>%
  mutate(ym = ym + month(1)) %>%
  right_join(well_daily, by = c("ym" = "date", "well")) %>%
  arrange(well, ym) %>%
  full_join(., getUnitVisitation(units = "BRCA", 
                                 startYear = 2000, endYear = 2023) %>%
              mutate(ym = ym(paste0(Year, "-", Month))) %>%
              select(ym, RecreationVisitors), by = "ym")

ggplot(data = joined_well_data) +
  geom_line(aes(x = ym, y = static_in)) +
  facet_wrap(~well, nrow = 2) +
  theme_bw() +
  xlab("Date") +
  ylab("Static Water Level (in)")

ggplot(data = filter(joined_well_data, !is.na(use_acre_feet))) +
  geom_line(aes(x = ym, y = as.numeric(use_acre_feet))) +
  facet_wrap(~well, nrow = 2) +
  theme_bw() +
  xlab("Date") +
  ylab("Water Use (Acre-Feet)")

ggplot() +
  geom_line(data = joined_well_data, 
            aes(x = ym, y = static_in)) +
  geom_line(data = filter(joined_well_data, !is.na(use_acre_feet)), 
            aes(x = ym, y = as.numeric(use_acre_feet)*10),
            color = "grey") +
  geom_line(data = filter(joined_well_data, !is.na(RecreationVisitors)), 
            aes(x = ym, y = as.numeric(RecreationVisitors)/10000),
            color = "blue") +
  facet_wrap(~well, nrow = 2) +
  theme_bw() +
  xlab("Date") +
  scale_y_continuous(name = "Static water level (in)",
                     sec.axis = sec_axis(~./10,
                                         name = "Water use (acre-feet)",
                                         breaks = seq(0, 15, 5))) +
  theme(axis.title.y.right = element_text(color="grey"),
        axis.title.y.left = element_text(color="black"))
```

Contributing area to the wells
```{r}
mapview(POD_park) + mapview(summarize(watersupply_watershed))
```

Watershed characteristics

Relationship between WATER SUPPLY and watershed characteristics

- GridMet (temp, precip, rh, P-PET?, other wbm vars)... what about with a lag?
- DayMet (temp, precip, rh, P-PET?, other wbm vars)... what about with a lag?
- StreamCat variables?
- Visitation and/or water use?
- Nearby weather station data
- Nearby stream data
- 

```{r}
gridmet_vars <- get_gridMET(path = "data/", 
                        park = "misc", 
                        aoi = final_aoi, 
                        vars = c("tmmx", "tmmn", "pr", "rmax", "rmin", "pet", "etr", "vpd", "vs"), 
                        start = "1979-01-01", 
                        end = "2022-12-31")
```




Relationship between NPS WBM characteristics





















```{r}
all_wbm_runoff <- list.files('data/misc/wbm_hist/', pattern = "*runoff", full.names = TRUE) %>%
  map_dfr(~read_csv(.)) %>%
  data.table::data.table() %>%
  mutate(year = lubridate::year(time),
         month = lubridate::month(time),
         ym = ym(substr(time, 1, 7))) %>%
  group_by(year, lat, lon) %>%
  summarize(mean = mean(val, na.rm = TRUE)) 
# st_as_sf(coords = c("lon", "lat"), crs = 4269)

animated_plot <- ggplot() +
  geom_point(data = all_wbm_runoff, aes(x = lon, y = lat, color = mean, size = 2)) +
  coord_quickmap(xlim = c(min(all_wbm_runoff$lon), max(all_wbm_runoff$lon)), 
                 ylim = c(min(all_wbm_runoff$lat), max(all_wbm_runoff$lat))) +
  labs(title = 'Mean annual runoff in areas of interest',
       subtitle = 'Year: {frame_time}',
       caption = 'Source: NPS Water Balance Model') +
  theme_minimal() +
  theme(legend.position = "bottom") +
  gganimate::transition_time(year) +
  scale_color_gradientn(colors = c("#B8C7F4", "#002FA7", "#FF69B4", "#C15A42"), breaks = c(10, 100, 1000))

num_years <- max(all_wbm_runoff$year) - min(all_wbm_runoff$year) + 1
gganimate::animate(animated_plot, nframes = num_years, fps = 2)

```

NPS tracks monthly total park vistors. Here we pull that information in for the park:

```{r}
visitors <- getUnitVisitation(units = "BRCA", startYear = 2000, endYear = 2023) %>%
  mutate(ym = ym(paste0(Year, "-", Month))) %>%
  select(ym, RecreationVisitors) %>%
  right_join(., well_data, by = "ym")

ggplot(data = visitors) +
  geom_point(aes(x=RecreationVisitors, y = use_acre_feet, color = well))
```

### Nearby stream gages

```{r}
# load in all wbm tables:
all_wbm_runoff <- list.files('data/misc/wbm_hist/', pattern = "*runoff", full.names = TRUE) %>%
  map_dfr(~read_csv(.)) %>%
  data.table::data.table() %>%
  mutate(year = lubridate::year(time),
         month = lubridate::month(time))

gage_wbm <- all_wbm_runoff %>% 
  # convert to shapefile:
  st_as_sf(coords = c("lon", "lat"), crs = 4269) %>%
  st_join(., nldi_watershed, left = FALSE) %>%
  select(site_no, x, y, year, month, var, val)

mapview(gage_wbm, zcol = "site_no")
# ... looks right!
```

```{r}
total_wbm_monthly <- gage_wbm %>%
  group_by(site_no, month, year) %>%
  summarize(total_runoff = sum(val))
```

```{r}
all_nwis_flow <- list.files("data/misc/nwis/dv/", full.names = TRUE) %>%
  map_dfr(~ read_csv(.) %>% 
            data.table::data.table() %>% 
            mutate(across(everything(), as.character))) %>%
  bind_rows() %>%
  mutate(dateTime = lubridate::ymd(dateTime),
         flow_cfs = as.numeric(X_00060_00003)) %>%
  left_join(st_drop_geometry(nldi_watershed), by = "site_no") %>%
  mutate(flow_mmd = (flow_cfs * 28.3168) / (drain_sqkm * 1e6 * 1e6))

monthly_flow <- all_nwis_flow %>%
  mutate(year = lubridate::year(dateTime),
         month = lubridate::month(dateTime),
         ym = ym(substr(dateTime, 1, 7))) %>%
  filter(year >= "1980" & year <= "2022") %>%
  group_by(site_no, month, year, ym) %>%
  summarize(mean_flow = mean(flow_mmd)) 

comparison <- inner_join(monthly_flow, total_wbm_monthly, by = c("site_no", "month", "year")) %>%
  mutate(season = case_when(month %in% c(12,1,2,3,4) ~ "Winter",
                            month %in% c(5,6) ~ "Runoff",
                            month %in% c(7,8,9,10,11) ~ "Baseflow"))

ggplot(data = comparison) +
  geom_point(aes(x = mean_flow, y = total_runoff, color = month)) +
  facet_wrap(~site_no, nrow = 2) +
  theme_bw()

# Find instances where the WBM says there is NO runoff, but 
# the data says otherwise...
no_runoff <- comparison %>%
  filter(total_runoff == 0)

ggplot(data = no_runoff) +
  geom_histogram(aes(x = mean_flow, fill = as.factor(month))) + 
  theme_bw()
```

How does precipitation compare to streamflow and well data?

```{r}
# Find NOAA weather stations near our areas of interest
noaa_data <- getNOAA(aoi = st_buffer(final_aoi, 0.1), park = "misc") %>%
  #              prcp = Precipitation (tenths of mm)
  #    	         snow = Snowfall (mm)
  # 	           snwd = Snow depth (mm)
  #              tmax = Maximum temperature (tenths of degrees C)
  #              tmin = Minimum temperature (tenths of degrees C)
  #              elevation = meters
  select(name, id, date, tmax, tmin, prcp, snow, snwd, elevation, geometry) %>%
  # Raw NOAA data is in "tenths of" units for temp and precip:
  mutate_at(c("tmax", "tmin", "prcp"), function(x) {x * 0.10})

noaa_data_sub <- noaa_data %>%
  data.table::data.table() %>%
  mutate(ym = ym(substr(date, 1, 7))) %>%
  filter(year(ym) >= 1980 & year(ym) <= 2022) %>%
  group_by(name, id, ym, geometry) %>%
  summarize(tmax = mean(tmax, na.rm = TRUE),
            tmin = mean(tmin, na.rm = TRUE),
            tmean = ((tmax+tmin)/2),
            prcp = sum(prcp, na.rm = TRUE),
            snow = sum(snow, na.rm = TRUE),
            snwd = max(snwd, na.rm = TRUE)) %>%
  st_as_sf()


noaa_data_daily <- noaa_data %>%
  data.table::data.table() %>%
  mutate(ym = ym(substr(date, 1, 7))) %>%
  filter(year(ym) >= 1980 & year(ym) <= 2022) %>%
  st_as_sf()

# ymd("2022-01-01") - ymd("1980-01-01")
# Time difference of 15341 days
# 15341 * 0.8 = 12273

# drop to sites that have more than 80% of the years
# noaa_years <- noaa_data %>%
#   data.table::data.table() %>%
#   mutate(ym = ym(substr(date, 1, 7))) %>%
#   filter(year(ym) >= 1980 & year(ym) <= 2022) %>%
#   group_by(name) %>%
#   filter(!is.na(prcp)) %>%
#   summarize(data = n_distinct(date))

ggplot(noaa_data_sub) +
  geom_point(aes(x = ym, y = prcp)) +
  facet_wrap(~name)
```

NWIS flow data

For each stream gage of interest, find the nearest weather station with the most complete data record available (at the time that the NWIS gage was running between 1980-2022). The stream gage also needs to have at least 80% complete data for its time period.
Here, I am using cross-correlation to explore the relationship and time lag between the two distinct time series. Cross-correlation helps identify if changes in one time series are followed by changes in another, with a certain time delay.

```{r}
monthly_flow_precip <- all_nwis_flow %>%
  mutate(year = lubridate::year(dateTime),
         month = lubridate::month(dateTime)) %>%
  filter(year >= "1980" & year <= "2022") %>%
  group_by(site_no, month, year) %>%
  summarize(mean_flow = mean(flow))

monthly_flow_timeline <- all_nwis_flow %>%
  mutate(dateTime = ymd(dateTime)) %>%
  filter(year(dateTime) >= 1980 & year(dateTime) <= 2022) %>%
  group_by(site_no) %>%
  summarize(min = min(dateTime),
            max = max(dateTime),
            possible_n = (as.numeric(ymd(max(dateTime)) - ymd(min(dateTime))) + 1),
            actual_n = n()) %>%
  filter(actual_n >= (possible_n * 0.8))

time_pd_merger <- function(site){
  
  bounds <- monthly_flow_timeline %>%
    dplyr::filter(site_no == site)
  
  nwis_loc <- nwis %>%
    dplyr::filter(site_no == site)
  
  precip_data <- noaa_data %>%
    filter(!is.na(prcp)) %>%
    group_by(name) %>%
    filter(date >= bounds$min & date <= bounds$max) %>%
    summarize(actual_n = n()) %>%
    filter(actual_n >= (bounds$possible_n * 0.8)) #%>%
  
  nwis_loc$nearest_station <- st_nearest_feature(nwis_loc, precip_data)
  
  nwis_loc <- nwis_loc %>%
    mutate(distance = sf::st_distance(nwis_loc, precip_data[nwis_loc$nearest_station,], by_element = TRUE),
           nearest_station = precip_data[nwis_loc$nearest_station,]$name)
  
  return(nwis_loc)
  
}

flows_n_noaa <- unique(monthly_flow_precip$site_no) %>%
  map_dfr(~time_pd_merger(.)) 

new <- left_join(monthly_flow, flows_n_noaa, by = "site_no") %>%
  inner_join(noaa_data_sub, by = c("nearest_station" = "name", "ym"))

ggplotly(
  ggplot() +
    geom_line(data = new, aes(x = ym, y = mean_flow), color = "#B8C7F4") +
    geom_line(data = new, aes(x = ym, y = prcp), color = "#002FA7") +
    facet_wrap(~site_no, nrow = 2) +
    theme_minimal()
)

corr_finder <- function(site){
  
  sub <- new %>%
    filter(site_no == site)
  
  precip <- ts(sub$prcp, start = 1)
  flow <- ts(sub$mean_flow, start = 1)
  
  cross_correlation <- ccf(flow, precip)
  
  # Plot the cross-correlation function
  plot(cross_correlation, main = "Cross-Correlation Function", xlab = "Lag", ylab = "Correlation")
  
}

```

Interpretation: 




NWIS flow data

For each stream gage of interest, find the nearest weather station with the most complete data record available (at the time that the NWIS gage was running between 1980-2022). The stream gage also needs to have at least 80% complete data for its time period.
Here, I am using cross-correlation to explore the relationship and time lag between the two distinct time series. Cross-correlation helps identify if changes in one time series are followed by changes in another, with a certain time delay.

```{r}
# Subset our NWIS data to the time period of interest:
daily_nwis_flow <- all_nwis_flow %>%
  filter(year(dateTime) >= "1980" & year(dateTime) <= "2022") 

# Find how much data is available during this time period for each site:
daily_nwis_timeline <- daily_nwis_flow %>%
  group_by(site_no) %>%
  filter(!is.na(flow_cfs)) %>%
  summarize(min = min(dateTime), #start date
            max = max(dateTime), #end date
            # how many days exist between the start and end date
            possible_n = (as.numeric(ymd(max(dateTime)) - ymd(min(dateTime))) + 1),
            # how many days we have a record for:
            actual_n = n()) %>%
  # select only sites who have at least 80% of days covered
  filter(actual_n >= (possible_n * 0.8))

time_pd_merger <- function(site){
  
  bounds <- daily_nwis_timeline %>%
    dplyr::filter(site_no == site)
  
  nwis_loc <- nwis %>%
    dplyr::filter(site_no == site)
  
  precip_data <- noaa_data %>%
    filter(!is.na(prcp)) %>%
    group_by(name) %>%
    filter(date >= bounds$min & date <= bounds$max) %>%
    summarize(actual_n = n()) %>%
    # must be at least an 80% overlap between USGS and NOAA datasets:
    filter(actual_n >= (bounds$possible_n * 0.8))
  
  # Find the nearest NOAA station that meets the minimum data requirements:
  nwis_loc$nearest_station <- st_nearest_feature(nwis_loc, precip_data)
  
  # Grab the actual distance between the nearest NOAA station and the USGS gage
  nwis_loc <- nwis_loc %>%
    mutate(distance_to_station = sf::st_distance(nwis_loc, precip_data[nwis_loc$nearest_station,], by_element = TRUE),
           nearest_station = precip_data[nwis_loc$nearest_station,]$name) %>%
    select(site_no, nearest_station, distance_to_station)
  
  return(nwis_loc)
  
}

flows_n_noaa <- unique(daily_nwis_flow$site_no) %>%
  map_dfr(~time_pd_merger(.)) 

new <- left_join(daily_nwis_flow, flows_n_noaa, by = "site_no") %>%
  inner_join(., noaa_data_daily, by = c("nearest_station" = "name", "dateTime" = "date"))

ggplotly(
  ggplot() +
    geom_line(data = new, aes(x = dateTime, y = flow_mmd * 10^12), color = "#B8C7F4") +
    geom_line(data = new, aes(x = dateTime, y = prcp), color = "#002FA7") +
    facet_wrap(~site_no, nrow = 2) +
    theme_minimal()
)

corr_finder <- function(site){
  
  sub <- new %>%
    filter(site_no == site) %>%
    select(site_no, dateTime, prcp, flow_mmd) %>%
    mutate(flow_mmd_filled = zoo::na.approx(flow_mmd),
           prcp_filled = ifelse(is.na(prcp), 0, prcp))
  
  precip <- ts(sub$prcp_filled, start = 1)
  flow <- ts(sub$flow_mmd_filled, start = 1)
  
  cross_correlation <- ccf(precip, flow)
  
  # Plot the cross-correlation function
  plot(cross_correlation, main = "Cross-Correlation Function", xlab = "Lag", ylab = "Correlation")
  
}



```


What about with the well data?

```{r}
# Subset our NWIS data to the time period of interest:
daily_well_static <- well_daily %>%
  filter(year(date) >= "1980" & year(date) <= "2022") 

# Find how much data is available during this time period for each site:
daily_well_timeline <- daily_well_static %>%
  group_by(well) %>%
  filter(!is.na(static_in)) %>%
  summarize(min = min(date), #start date
            max = max(date), #end date
            # how many days exist between the start and end date
            possible_n = (as.numeric(ymd(max(date)) - ymd(min(date))) + 1),
            # how many days we have a record for:
            actual_n = n()) %>%
  mutate(completeness = 100 * (actual_n/possible_n))

time_pd_merger <- function(site){
  
  bounds <- daily_well_timeline %>%
    dplyr::filter(well == site)
  
  nwis_loc <- nwis %>%
    dplyr::filter(well == site)
  
  precip_data <- noaa_data %>%
    filter(!is.na(prcp)) %>%
    group_by(name) %>%
    filter(date >= bounds$min & date <= bounds$max) %>%
    summarize(actual_n = n()) %>%
    # must be at least an 80% overlap between USGS and NOAA datasets:
    filter(actual_n >= (bounds$possible_n * 0.8))
  
  # Find the nearest NOAA station that meets the minimum data requirements:
  nwis_loc$nearest_station <- st_nearest_feature(nwis_loc, precip_data)
  
  # Grab the actual distance between the nearest NOAA station and the USGS gage
  nwis_loc <- nwis_loc %>%
    mutate(distance_to_station = sf::st_distance(nwis_loc, precip_data[nwis_loc$nearest_station,], by_element = TRUE),
           nearest_station = precip_data[nwis_loc$nearest_station,]$name) %>%
    select(site_no, nearest_station, distance_to_station)
  
  return(nwis_loc)
  
}

flows_n_noaa <- unique(daily_nwis_flow$site_no) %>%
  map_dfr(~time_pd_merger(.)) 

new <- left_join(daily_nwis_flow, flows_n_noaa, by = "site_no") %>%
  inner_join(., noaa_data_daily, by = c("nearest_station" = "name", "dateTime" = "date"))

ggplotly(
  ggplot() +
    geom_line(data = new, aes(x = dateTime, y = flow_mmd * 10^12), color = "#B8C7F4") +
    geom_line(data = new, aes(x = dateTime, y = prcp), color = "#002FA7") +
    facet_wrap(~site_no, nrow = 2) +
    theme_minimal()
)

corr_finder <- function(site){
  
  sub <- new %>%
    filter(site_no == site) %>%
    select(site_no, dateTime, prcp, flow_mmd) %>%
    mutate(flow_mmd_filled = zoo::na.approx(flow_mmd),
           prcp_filled = ifelse(is.na(prcp), 0, prcp))
  
  precip <- ts(sub$prcp_filled, start = 1)
  flow <- ts(sub$flow_mmd_filled, start = 1)
  
  cross_correlation <- ccf(precip, flow)
  
  # Plot the cross-correlation function
  plot(cross_correlation, main = "Cross-Correlation Function", xlab = "Lag", ylab = "Correlation")
  
}



```

