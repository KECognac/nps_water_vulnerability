---
title: "Retrieve and Process Climate Centroid Data"
author: "Caitlin Mothes and Katie Willi"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: paper
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      error = FALSE,
                      message = FALSE)
                      #cache = TRUE)

source("setup.R")
```

# Workflow to process MACA climate centroid data

**Codebase modified from <https://github.com/nationalparkservice/CCRP_automated_climate_futures>, led by Amber Runyon.**

Function to pull in all park centroid files:

```{r}
get_files <- function(park) {
  walk(list.files(
    paste0("data/park/", park, "/centroid/climate"),
    full.names = TRUE
  ),
  function(x) {
    tmp <- read_csv(x)
    # hacky way to pull filename to assign to env object
    name <-  str_sub(x, 33,-5)
    assign(name, tmp, envir = .GlobalEnv)
  })
}



```

### Get data for park:

```{r}
park <- "BRCA"

## get all files
get_files("BRCA")

## create list of future and historic dfs
## MAKE SURE no other objects with '_future' or '_historical' in their names
future_dfs <- mget(ls(pattern = "_future"))

historic_dfs <- mget(ls(pattern = "_historical"))

```

### Set up some parameters?

```{r}
CFs_all <- c("Warm Wet", "Hot Wet", "Central", "Warm Dry", "Hot Dry")

##Color schemes

#Colors for CF values plotted side by side (match order of CFs vector)
colors5 <-  c("#6EB2D4", "#05689F", "#F6B294", "#CA0020","grey")
colors5.2 <- c("#6EB2D4", "#05689F", "grey", "#F6B294", "#CA0020")

# Switch for using Tercek csvs or downloading own data:
centroids_csv <- "Y" 
# Switch for method Indiv_method = c("corner", "pca"):
Indiv_method <- "pca" 
# Percentage of models to drop from ranking:
Percent_skill_cutoff = .1
# Indicates whether Q/I present at bottom of plot to ID CF method used:
MethodCaption = "Y" 


# Threshold percentages for defining Climate futures. Default low/high:  0.25, 0.75
CFLow = 0.25     
CFHigh = 0.75

# Quantiles for temperature threshold calculations
QuantileLow = 0.05
QuantileHigh = 0.95

HotTemp = 95
ColdTemp = 32
PrecipThreshold = 0.05
```

#### Variable calculations:

```{r}
TFtoC <- function(T){(T-32)/1.8}

# VP from FAO -  https://www.fao.org/3/x0490e/x0490e07.htm
# could also use Buck 1981 for 'improved':
# Buck: VPDsat (mb) = (1.0007 + (3.46 * 10^-6 * P)) * 6.1121 * exp((17.50 * T)/(T+240.87))
# where T is deg C and P is atm pressure mb. Above for P > 800 (correction is minimal)
# Zackman re: Ragwala uses: Es = 611.6441 * 10^[(7.591386*T)/(240.7263+T)] where Tavg.
# Shelley sent Vaisala- to use Tavg for 611.6441 parameter - for 020 to +50 C.
# VPsatT = saturation VP @ T deg C [kPa]
# VPD [kPa]
VPsatT <- function(T){0.6108 * exp((17.27 * T)/(T + 237.3))}   

VPD <- function(TminF, TmaxF, RHmin, RHmax){
  Tmin <- TFtoC(TminF); Tmax <- TFtoC(TmaxF)
  es <- (VPsatT(Tmin)+VPsatT(Tmax))/2
  ea <- (VPsatT(Tmin)*RHmax*.01 + VPsatT(Tmax)*RHmin*.01)/2
  es - ea   }  # end VPD  
```

### Clean data

Following methods in draft Climate report from Dave, "Methods for assessing climate change exposure for national park planning"; Runyon et al. 2023.

```{r}
# future data, filter to 2035-2065 (2050 mean)
future_all <- future_dfs[[1]] %>%
  dplyr::rename(precip_in = `Precip (in)`,
         tmin_f = `Tmin (F)`,
         tmax_f = `Tmax (F)`,
         rhmax = `RHmax (%)`,
         rhmin = `RHmin(%)`,
         tavg_f = `Tavg (F)`) %>% 
  mutate(
    year = format(Date, "%Y"),
    VPD = VPD(tmin_f, tmax_f, rhmin, rhmax), # do we need vapor pressure??
    DOY = yday(Date)
  ) %>% 
  filter(year %in% 2035:2065)
  

# historic data, filter to 1979-2012 baseline
historic_all <- historic_dfs[[1]] %>%
 dplyr::rename(precip_in = `Precip (in)`,
         tmin_f = `Tmin (F)`,
         tmax_f = `Tmax (F)`,
         rhmax = `RHmax (%)`,
         rhmin = `RHmin(%)`,
         tavg_f = `Tavg (F)`) %>% 
  mutate(
    year = format(Date, "%Y"),
    VPD = VPD(tmin_f, tmax_f, rhmin, rhmax),
    DOY = yday(Date)
  ) %>% 
  filter(year %in% 1979:2012)
```

### Remove low skill models

```{r}
# Determine low-skill models using list created from Rupp et al. 2016 (this is from the CCRP team)

# assign region of park (one of SWR, SER, PWR or 'mean' if none of these)
region <- "SWR"


low_skill_models <-
  read_delim('data/GCM_skill_by_region.txt') %>%
  filter(if (region %in% Region) {
    Region == region
  } else {
    Region == "mean"
  }) %>%
  # remove period at end of GCM names (will need later)
  mutate(GCM = str_sub(GCM, 1, -2)) %>% 
  # Worse models have higher value rank
  slice_max(n = length(unique(future_all$GCM)) / 2 * Percent_skill_cutoff,
            order_by  = Rank)

```

### Set up for PCA

Calculate all baseline values, averages, and change from baseline to 2050 average.

```{r}

# baseline means from historic data
baseline <- historic_all %>% 
  summarise(baseline_pr = mean(precip_in),
            baseline_tmax = mean(tmax_f),
            baseline_tmin = mean(tmin_f),
            baseline_tavg = mean(tavg_f),
            baseline_rhmax = mean(rhmax),
            baseline_rhmin = mean(rhmin))


# future means for each GCM
future_means <- future_all %>% 
  group_by(GCM) %>% 
  summarise_at(vars(precip_in:tavg_f), mean, na.rm = TRUE) %>% 
  # add delta columns using baseline values
  mutate(delta_pr = precip_in - baseline$baseline_pr,
         delta_tmax = tmax_f - baseline$baseline_tmax,
         delta_tmin = tmin_f - baseline$baseline_tmin,
         delta_tavg = tavg_f - baseline$baseline_tavg,
         delta_rhmax = rhmax - baseline$baseline_rhmax,
         delta_rhmin = rhmin - baseline$baseline_rhmin)


```

## PCA Method

### #1 : using just change in precip and avg temp (Runyon et al. methods)

```{r}
# need this for plotting pca
library(ggfortify)

#Remove low skill GCMs and select variables of interest
future_pca_1 <- future_means %>% 
  separate_wider_delim(GCM, 
                       names = c("GCM_only", "RCP"),
           delim = ".",
           cols_remove  = FALSE) %>% 
  filter(!GCM_only %in% low_skill_models$GCM) %>% 
  dplyr::select(GCM, delta_pr, delta_tavg) %>% 
  column_to_rownames(var = 'GCM')


pca_1 <- prcomp(future_pca_1, center = TRUE, scale. = TRUE) 

# quick plot
autoplot(pca_1, data = future_pca_1, loadings = TRUE,label=TRUE)

# get dataframe
pca_1_df <- as.data.frame(pca_1$x)
#write_csv(pca_1_df, paste0(OutDir, "/PCA-loadings.csv"))

#Take the min/max of each of the PCs
PCs <- pca_1_df %>% 
 filter(PC1 == min(PC1) |
        PC1 == max(PC1) |
        PC2 == min(PC2) |
        PC2 == max(PC2))


# PCs <- rbind(data.frame(GCM = c(
#   rownames(pca_1_df)[which.min(pca_1_df$PC1)], rownames(pca_1_df)[which.max(pca_1_df$PC1)]
# ), PC =
#   "PC1"),
# data.frame(GCM = c(
#   rownames(pca_1_df)[which.min(pca_1_df$PC2)], rownames(pca_1_df)[which.max(pca_1_df$PC2)]
# ), PC =
#   "PC2"))

#Assigns CFs to diagonals
diagonals <-
  rbind(
    data.frame(CF = CFs_all[c(1, 5)], diagonals = factor("diagonal1")),
    data.frame(CF = CFs_all[c(4, 2)], diagonals = factor("diagonal2"))
  )


PCA <-
  CF_GCM %>% filter(GCM %in% PCs$GCM) %>% left_join(diagonals, by = "CF") %>% right_join(PCs, by =
                                                                                           "GCM")

ID.redundant.gcm <- function(PCA){
  redundant.diag=count(PCA,diagonals)$diagonals[which(count(PCA,diagonals)$n==1)] #ID redundant diagonal
  PC.foul = PCA$PC[which(PCA$diagonals == redundant.diag)] #ID which PC has the redundant diagonal
  PCA$GCM[which(PCA$PC == PC.foul & PCA$GCM != PCA$GCM[which(PCA$diagonals == redundant.diag)])] #ID GCM that is in both the  redundant diagonal and the duplicative PC
}

Future_Means %>% mutate(pca = ifelse(GCM %in% PCs$GCM[which(PCs$PC=="PC1")], as.character(CF), #assign PCs to quadrants and select those gcms
                                     ifelse(GCM %in% PCs$GCM[which(PCs$PC=="PC2")], as.character(CF),NA))) -> Future_Means #Future_Means

if(length(
  setdiff(CFs_all[CFs_all != "Central"],Future_Means$pca)) > 0){ #if a quadrant is missing 
  Future_Means$pca[which(Future_Means$corners == setdiff(CFs_all[CFs_all != "Central"],Future_Means$pca))] = setdiff(CFs_all[CFs_all != "Central"],Future_Means$pca) #assign corners selection to that CF
  if(nrow(PCA[duplicated(PCA$GCM),]) > 0) { #If there is a redundant GCM
    Future_Means$pca = Future_Means$pca #Do nothing - otherwise end up with empty quadrant. This line could be removed and make the previous statment inverse but it makes it more confusing what's gonig on that way
  } else{
    Future_Means$pca[which(Future_Means$GCM == ID.redundant.gcm(PCA))] = NA #Removes the GCM that is in redundant diagonal
  }
}
# KW: Which climate models do we want to use... do we select by PCA or Corner method (i.e., whatever `Indiv_method` is 
# KW: at the top of katie_wbm.R)
Future_Means <- Future_Means %>% mutate(select = eval(parse(text=paste0("Future_Means$",Indiv_method))))
WB_GCMs <- Future_Means %>% drop_na(select) %>% dplyr::select(c(GCM,CF))

rm(lx,ux,ly,uy,ww,wd,hw,hd, pts, FM, pca,pca.df,PCs,diagonals,PCA) 

```

**Running notes for arguments to go in eventual function:**

-   park

-   region (three letter NPS region of park)

-   future year range (min, max)

-   historic year range (min, max; note 1979 is min)

-   percent low skill cut off (default from Amber's code is 10%)

CORNERS METHOD

```{r}
#### Set limits for CF classification
Pr0 = as.numeric(quantile(Future_Means$DeltaPr, 0))
Pr25 = as.numeric(quantile(Future_Means$DeltaPr, CFLow))
PrAvg = as.numeric(mean(Future_Means$DeltaPr))
Pr75 = as.numeric(quantile(Future_Means$DeltaPr, CFHigh))
Pr100 = as.numeric(quantile(Future_Means$DeltaPr, 1))
Tavg0 = as.numeric(quantile(Future_Means$DeltaTavg, 0))
Tavg25 = as.numeric(quantile(Future_Means$DeltaTavg, CFLow)) 
Tavg = as.numeric(mean(Future_Means$DeltaTavg))
Tavg75 = as.numeric(quantile(Future_Means$DeltaTavg, CFHigh))
Tavg100 = as.numeric(quantile(Future_Means$DeltaTavg, 1))

#### Designate Climate Future
Future_Means$CF1 = as.numeric((Future_Means$DeltaTavg<Tavg & Future_Means$DeltaPr>Pr75) | Future_Means$DeltaTavg<Tavg25 & Future_Means$DeltaPr>PrAvg)
Future_Means$CF2 = as.numeric((Future_Means$DeltaTavg>Tavg & Future_Means$DeltaPr>Pr75) | Future_Means$DeltaTavg>Tavg75 & Future_Means$DeltaPr>PrAvg)
Future_Means$CF3 = as.numeric((Future_Means$DeltaTavg>Tavg25 & Future_Means$DeltaTavg<Tavg75) & (Future_Means$DeltaPr>Pr25 & Future_Means$DeltaPr<Pr75))
Future_Means$CF4 = as.numeric((Future_Means$DeltaTavg<Tavg & Future_Means$DeltaPr<Pr25) | Future_Means$DeltaTavg<Tavg25 & Future_Means$DeltaPr<PrAvg)
Future_Means$CF5 = as.numeric((Future_Means$DeltaTavg>Tavg & Future_Means$DeltaPr<Pr25) | Future_Means$DeltaTavg>Tavg75 & Future_Means$DeltaPr<PrAvg)


#Assign full name of climate future to new variable CF
Future_Means$CF[Future_Means$CF1==1]=CFs_all[1]
Future_Means$CF[Future_Means$CF2==1]=CFs_all[2]
Future_Means$CF[Future_Means$CF3==1]=CFs_all[3]
Future_Means$CF[Future_Means$CF4==1]=CFs_all[4]
Future_Means$CF[Future_Means$CF5==1]=CFs_all[5]

#     Remove extraneous Climate Future columns
Future_Means$CF1 = NULL
Future_Means$CF2 = NULL
Future_Means$CF3 = NULL
Future_Means$CF4 = NULL
Future_Means$CF5 = NULL

#     Add column with emissions scenario for each GCM run
Future_Means$emissions[grep("rcp85",Future_Means$GCM)] = "RCP 8.5"
Future_Means$emissions[grep("rcp45",Future_Means$GCM)] = "RCP 4.5"

#### Select Corner GCMs
lx = min(Future_Means$DeltaTavg)
ux = max(Future_Means$DeltaTavg)
ly = min(Future_Means$DeltaPr)
uy = max(Future_Means$DeltaPr)

  #convert to points
ww = c(lx,uy)
wd = c(lx,ly)
hw = c(ux,uy)
hd = c(ux,ly)

pts <- Future_Means %>% filter(str_detect(GCM, paste(low_skill_models$GCM,collapse = '|'), negate = TRUE))

  #calc Euclidian dist of each point from corners
# KW: "corners" being the extremes: ww, wd, hw, hd
pts$WW.distance <- sqrt((pts$DeltaTavg - ww[1])^2 + (pts$DeltaPr - ww[2])^2)
pts$WD.distance <- sqrt((pts$DeltaTavg - wd[1])^2 + (pts$DeltaPr - wd[2])^2)
pts$HW.distance <- sqrt((pts$DeltaTavg - hw[1])^2 + (pts$DeltaPr - hw[2])^2)
pts$HD.distance <- sqrt((pts$DeltaTavg - hd[1])^2 + (pts$DeltaPr - hd[2])^2)

pts %>% filter(CF == "Warm Wet") %>% slice(which.min(WW.distance)) %>% .$GCM -> ww
pts %>% filter(CF == "Warm Dry") %>% slice(which.min(WD.distance)) %>% .$GCM -> wd
pts %>% filter(CF == "Hot Wet") %>% slice(which.min(HW.distance)) %>% .$GCM -> hw
pts %>% filter(CF == "Hot Dry") %>% slice(which.min(HD.distance)) %>% .$GCM -> hd

Future_Means <- Future_Means %>% mutate(corners = ifelse(GCM == ww,"Warm Wet",
                                         ifelse(GCM == wd, "Warm Dry",
                                                ifelse(GCM == hw, "Hot Wet",
                                                       ifelse( GCM == hd, "Hot Dry",NA)))))

## not sure where this fits in, I think prep for PCA in Amber's code

FM <- Future_Means %>% dplyr::select("GCM","DeltaPr","DeltaTavg") %>%  
  filter(str_detect(GCM, paste(low_skill_models$GCM,collapse = '|'), negate = TRUE)) %>% 
  remove_rownames %>% column_to_rownames(var="GCM") 
CF_GCM = data.frame(GCM = Future_Means$GCM, CF = Future_Means$CF)
```
