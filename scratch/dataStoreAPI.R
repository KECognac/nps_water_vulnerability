# search and download data from IRMA Data Store with REST Api

# https://irmaservices.nps.gov/datastore/v4/documentation/datastore-api.html#/


# SETUP ------------------------------------------------------------------------
# use httr and jsonlite to make GET requests, parse results and page through data
library(httr)
library(jsonlite)
library(dplyr)
library(purrr)
library(readr)



#set base URL
call <- "https://irmaservices.nps.gov/datastore/v4/rest"



# TESTING ---------------------------------------------------------------------

# test call using single reference ID (use DEVA HIS)
deva_his <- httr::GET(paste0(call, "/Profile/2286975"))

#structure of content
str(content(deva_his))

#convert content to text
deva_his_txt <- content(deva_his, "text", encoding = "UTF-8")

#parse data in JSON
deva_his_json <- fromJSON(deva_his_txt,
                          flatten = TRUE)


# extract referenceID (for example, even though we already know this)
refID <- deva_his_json$referenceId


# search linked resources to get dataset id for download
deva_his_res <- httr::GET(paste0(call, "/Reference/", refID, "/DigitalFiles"))

#extract resourceID
resContent <- content(deva_his_res)

resID <- resContent[[1]]$resourceId


#download file
## get meta on file like file type
data <- httr::GET(paste0(call, "/DownloadFile/", resID))

download.file(paste0(call, "/DownloadFile/", resID), destfile = "data/deva_his.zip", method = "curl")


# DEVA -----------------------------------------------------------
# test workflow with DEVA first, then write it as a loop for all parks

## Search all park info for DEVA

deva <- httr::GET(paste0(call, "/QuickSearch?q=DEVA&units=DEVA&top=1000")) 
#specify unit (park code) and top sets the number of entries per page
#call will only return a single page so set top high to get all entries


#view content structure of returned call
str(content(deva))

# convert content to text
deva_text <- content(deva, "text", encoding = "UTF-8")

#parse data in JSON
deva_json <- fromJSON(deva_text, flatten = TRUE)

#convert items to data.frame
deva_df <- as_tibble(deva_json$items)

#filter out dockets and geospatial datasets where file count is >= 1

deva_df_clean <- deva_df %>% 
  filter(referenceType %in% c("Docket","Geospatial Dataset")) %>% 
  mutate(unit = "DEVA") %>% 
  relocate(unit) %>% 
  select(-newestVersion)





# FULL WORKFLOW --------------------------------------------------------------

park <- c("DEVA", "JOTR", "MOJA")


final_df <- vector("list", length = length(park))


for (i in 1:length(park)){
  
  # Search all park info
  dat <- httr::GET(paste0(call, "/QuickSearch?q=", park[i], "&units=", park[i], "&top=1000")) 
  #specify unit (park code) and top sets the number of entries per page
  #call will only return a single page so set top high to get all entries
  
  
  # convert content to text
  dat_text <- content(dat, "text", encoding = "UTF-8")
  
  #parse data in JSON
  dat_json <- fromJSON(dat_text, flatten = TRUE)
  
  #convert items to data.frame
  dat_df <- as_tibble(dat_json$items)
  
  #filter out dockets and geospatial datasets where file count is >= 1
  
  dat_df_clean <- dat_df %>% 
    filter(referenceType %in% c("Docket","Geospatial Dataset")) %>% 
    mutate(unit = park[i]) %>% 
    relocate(unit) %>% 
    select(-newestVersion)
  
  
  #create empty vector to fill in resourceID
  resID <- vector("character", length = nrow(dat_df_clean))
  
  #now get resourceID (dataset download ID) for each item
  for (j in 1:nrow(dat_df_clean)){
    
    refID <- as.character(dat_df_clean[j, "referenceId"])
    
    res <- httr::GET(paste0(call, "/Reference/", refID, "/DigitalFiles"))
    
    #extract resourceID
    resContent <- content(res)
    
    # if no file, no resourceID so assign NA
    if(length(resContent) == 0){
      
      resID[j] <- NA
    
      } else {
      
        resID[j] <- map(resContent, `[[`, 2) %>% 
          unlist() %>% 
          paste(collapse = ",")
    }
    

  }
  
  final_df[[i]] <- dat_df_clean %>% 
    mutate(resourceId = resID)
    
  
  
  print(i)
  
}
  

final_df <- bind_rows(final_df)

write_csv(final_df, file = "data/datastore_parkMeta.csv")




